\documentclass[]{auvsi_doc}
\setkeys{auvsi_doc.cls}{
	AUVSITitle={Requirements Matrix},
	AUVSIRevision=0.1,0.2,1.0
	AUVSIDescription={Initial Draft},
	AUVSIAuthor={Kameron Eves},
	AUVSIChecker={},
	AUVSILogoPath={./figs/logo.pdf},
	AUVSIDocID={RM-001}
}

% include extra packages, if needed

% Remove Heading Numbers
\setcounter{secnumdepth}{0}

\begin{document}
\CapstoneTitlePage
% document contents

\section{Purpose}
\section{Market Representatives}
The market for this product consists of the Association for Unmanned Vehicle Systems International (AUVSI) Student Unmanned Aerial Systems (SUAS) Competition judges.
They will determine the ultimate success of the product through its performance in the AUVSI SUAS Competition.
These judges provide a set of competition requirements that are our primary source for market requirements.
On September 14th, 2018, the competition requirements for the 2019 competition year were released.
From September 14th, 2018 to October 4th, 2018, the competition judges provide a comment period where team members request clarification on the requirements. On October 4th, a final copy of the rules is released, and recourse continues to be provided for requesting further rules clarification.
The current competition requirements and the comment period are our primary means of determining the market requirements.
In addition, our coach, Dr. Ning, and our sponsor, Dr. McLain, provide on-campus market representation, as well as feedback on our capstone documents. Both individuals have been involved with competing teams in the past, so they have a good idea of what the major competition expectations are.

\section{Development of the Opportunity}

To determine the market requirements, we started with a detailed study of the AUVSI SUAS competition requirements.
We determined the most crucial portions of the competition requirements by benchmarking individual teams' performance in previous competitions (recorded in DJ-003 and DJ-006), and by breaking down the relative scoring weights of the competition requirements (recorded in DJ-004). 
Using these studies, we prepared a set of requirements ready for feedback from our on-campus market representatives.
On October 1st, 2018, we presented the requirements to Dr. Ning and Dr. McLain.
Overall, their feedback was positive. They also encouraged us to refine the requirements through developing subsystem requirements in the future.

\section{Justification and Validation of Key Success Measures}
Using the information obtained from the market representatives, we created a list of key success measures which we will use as a measuring stick to validate our product. We feel strongly that if we do not achieve at least fair performance in all of these key success measures, then our product will be unsatisfactory and we will consider the project a failure. This is primarily because the unmanned aerial system (UAS) would not meet the market's needs, as it would not meet a level of competitiveness equivalent to last year's team. On the other hand, we feel equally as strongly that if we achieve excellent performance in most, or preferably all, of these key success measures, then our product will represent outstanding work that precisely fulfills the needs of the market. What follows is an enumeration and justification of the key success measures. 

Because our product is inherently for a competition, most of these key success measures use the distribution of possible points as a framework. A break down of the point distribution can be found in DJ-004. Another important source for these decisions was the performance of last year's BYU team as well as the top 5 teams. This helped us determine what was possible and what isn't possible considering the resources available. The results of this analysis can be found in DJ-003 and DJ-006.

\begin{itemize}
\item \textbf{Obstacles Hit} This constitutes 20\% of the points our UAS can obtain. This year, the judges have increased the number of obstacles that need to be avoided from 20 to 30. Last year's team hit 3 obstacles and only 48\% of all teams in the competition were able to avoid any obstacles. 
	\begin{itemize}
	\item  \underline{Fair: 5 Obstacles} -  If last years performance is scaled for the increased number of obstacles (a 50\% increase) we would need to avoid 5 obstacles to match last years performance. However, it is likely that the number of obstacles hit increases exponentially with the number of obstacles in the competition. This is because the likelihood of avoiding obstacles by luck decreases exponentially. Therefore, avoiding 5 obstacles might be numerically equivalent to last years performance, but could still indicate some small improvement. Thus we choose 5 obstacles as a fair performance. Anything less then 5 would indicate a failure to improve last years system.
	\item \underline{Good: 3 Obstacles} -  As in all aspects of the competition, we hope to improve upon last years performance. Therefore, we feel that avoiding 3 obstacles (the same number as last year) constitutes merely a good performance. Again accounting for the increased number of obstacles, this would be a small improvement from last year.
	\item \underline{Excellent: 1 Obstacle} -  Decreasing the number of obstacles hit from 3 to 1 represents a marked improvement from last year. As such we have set this as excellent performance.
	\item \underline{Stretch: 0 Obstacles} - The ideal is to avoid all obstacles. While this is difficult, we do feel it is possible and so have set this as our stretch goal.
	\end{itemize}
\item \textbf{Waypoint Proximity} Autonomous flight of a waypoint path constitutes 20\% of the points our UAS can obtain. Among other things, points are awarded for how close the UAS comes to each waypoint. Only 56\% of all teams were able to get points for autonomously flying a waypoint path. Last year's team averaged 16 feet from the waypoints.  As mentioned previously, there has been a significant increase in the number of obstacles. Additionally, this year we will need to increase the size and speed of our aircraft in order to carry an increased payload. Both of these facts will introduce more error into our flight path. 
	\begin{itemize}
	\item  \underline{Fair: 25 feet} -  Due to this year's increased difficulty, repeating last years results would actually indicate an improvement. After consulting with our market representatives, we have decided that anything below 25 feet would mark no improvement over last year's performance and would show no improvement in our path planning or flight control. Thus, we have chosen 25 feet to be the limit of a fair performance. 
	\item \underline{Good: 20 feet} -  To improve upon last years system we will need to make changes to the path planner and the flight control. An average of 20 feet from the waypoints, would only show improvement in one of these areas. This 20 feet indicates only a good performance.
	\item \underline{Excellent: 15 feet} -  Because of this year's increased difficulty, repeating last years performance would be an excellent performance and would show significant improvement in both the airframe and path planner.
	\item \underline{Stretch: 10 feet} -  The ideal is of course 0 feet away from the waypoint. However, due to uncontrollable factors such as weather conditions and our limited resources we feel that this ideal is unrealistic. Therefore, we have set our stretch goal to something we feel is possible, but very difficult. This stretch goal would be a very large improvement over last year and would reward us with 90\% of the points possible for this portion of the competition. 
	\end{itemize}
\item \textbf{Object Characteristics Identified} Identifying the characteristics of several objects on the ground constitutes 20\% of the points our UAS can obtain. Points are awarded for the number of characteristics correctly reported. These characteristics include the object's color, the object's shape, the alphanumeric character on the object, and the object's location. Last year's team correctly identified only 23\% of the possible characteristics. Only 17\% of all teams in the competition received points for identifying any characteristics.
	\begin{itemize}
	\item  \underline{Fair: 20\%} -  Below 20\% would mark no improvement over last years performance. As such this is our limit for a fair performance.
	\item \underline{Good: 30\%} -  While this level would indicate improvement over last year's performance, it would not indicate significant improvement. There are several difficult, but obvious, changes that could result in this improvement. Such changes could include finding and fixing a known geolocation bug, as well as improving the usability of the vision ground station GUI. This proverbial low hanging fruit would constitute only a good performance.
	\item \underline{Excellent: 40\%} - Achieving this level would mean nearly doubling last year's performance. To achieve this we would need to identify at least one more characteristic per object. Whether this is accomplished autonomously or manually, this will be difficult to achieve and will require innovative changes to the current system. As such we have labeled this excellent performance.
	\item \underline{Stretch: Autonomous Detection} - Additional points are awarded for the UAS autonomously identifying and reporting object characteristics. This is very difficult. Last year's team did not even attempt this. Thus we have set this as our stretch goal.
	\end{itemize}
\item \textbf{Airdrop Accuracy} Payload delivery constitutes 20\% of the points our UAS can obtain. Last year's team received no points for this portion of the competition. However, this was because of other factors not relevant to the payload delivery. Because we can not compare our results to last year's results, we have set these goals primarily off of feedback received from the judges in recent weeks. This year, the payload delivery has increased significantly in difficulty. Last year only involved dropping a water bottle. This year includes dropping an autonomous, remote-controlled (RC) car capable of driving itself to a specified location. Accuracy for our drop will be extremely difficult to achieve. Our payload will need to land softly to avoid breaking. However, most mechanisms for ensuring a soft landing would also involve significant decreases in the accuracy of the drop (e.g. a drifting parachute). Last year, only 29\% of teams received points for payload delivery. 
	\begin{itemize}
	\item  \underline{Fair: 75 feet} -  No points are awarded for an airdrop with an accuracy of less then 75 feet. As such this is our lower acceptable limit.
	\item \underline{Good: 50 feet} -  The next tiered level of performance given us by our market representatives is from 50 feet to 75 feet. Any accuracy within this range would reward us 25\% of the points for this portion of the competition.
	\item \underline{Excellent: 25 feet} -  An accuracy of less then 25 feet would result in 50\% of the possible points for this portion of the competition being awarded to us. If our stretch goal is not achieved, then this is the maximum amount of points we can obtain. Thus this is our excellent performance.
	\item \underline{Stretch: 5 feet} -  Our market representatives have indicated that their ideal is an UAS capable of dropping it's payload within 5 feet of the designated target. The judges have indicated that this would result in full points awarded for this section of the competition. While possible, we do not feel that this is feasible with our given resources. As such we have set it as our stretch goal.
	\end{itemize}
\item \textbf{Number of Manual Takeovers} The ethos of this competition is autonomy. Autonomous flight directly constitutes 8\% of the points our UAS can obtain. However, most other tasks can not be completed without autonomous flight. During the competition, if our autopilot failed in any way, it would necessitate a manual takeover. A manual takeover is when our safety pilot performes an RC override and pilots the aircraft manually for a short time. Doing so results in a points penalty equal to 10\% of the autonomous flight points. Last years team only needed 1 manual takeover; however, we feel that this is not a good indication of how many takeovers we'll need. The code base for our system is complex and interconnected, as such every change in our software (of which we will be making many) could, if every effect of the change is not accounted for, cause a manual takeover. Excluding our predecessors, there is no data available for the number of manual takeovers needed by teams last year.
	\begin{itemize}
	\item  \underline{Fair: 3 Takeover} -  Any number of manual takeovers more then this would be unsatisfactory and indicate an inability to autonomously control the UAS.
	\item \underline{Good: 2 Takeovers} -  This is one more takeover then last year and would indicate that we made the same number of system critical mistakes as last year's team. 
	\item \underline{Excellent: 1 Takeovers} -  This would equal last year's results, however, it would also indicate that no of our changes resulted in a system critical error.
	\item \underline{Stretch: 0 Takeover} - This is of course the ideal, but very difficult to achieve as it would require finding and correcting hundreds of bugs.
	\end{itemize}
\end{itemize}

Just as important as the key success measures are several other features of the aircraft. Our team is inherently a competition team. As such, our main goal and biggest indicator of success is how we perform in the competition. However, our final place in the competition was excluded from our key success measures intentionally because we can not control how the other teams perform. As such we could perform very well compared to other teams, but still not have a satisfactory aircraft. Therefore, despite the fact that this is the primary goal of our aircraft, we have purposely excluded it from our key success measures. Successfully achieving excellent performance in our key success measures should ensure excellent performance the competition regardless of the performance of the other teams.

\section{Validation of the Completed Sections of the Requirements Matrix}
It is vital for requirements matrices to be developed in consultation with the market representatives. Our requirements matrix was not developed in a vacuum. Its requirements closely mirror the desires of the market. This was achieved through communication with the judges through the rules. The rules are divided into several sections which list the places where points are to be obtained. The titles of these rules were used as market statements, which we turned into market requirements. The points distribution within these sections was used to develop requirement measures. These measures indicate whether or not our product is capable of meeting the market requirements. The higher and lower acceptable values were chosen from the points distribution itself. Finally, and most importantly, we confirmed our results with Dr. McLain. Dr. McLain indicated that each requirement was good and correctly differentiated a successful product from a failed product. As an aside, the key success measures listed above were developed in tandem with the market requirements. An effort was made to ensure that at-least one key success measure can be used somewhat comprehensively to measure each market requirement. If we successfully achieve excellent performance in the key success measures then we will have created a product which the market also feels is excellent.

\end{document}
