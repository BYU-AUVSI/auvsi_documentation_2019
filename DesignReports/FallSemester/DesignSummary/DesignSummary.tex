\documentclass[]{auvsi_doc}
\setkeys{auvsi_doc.cls}{
	AUVSITitle={Design Summary},
%	AUVSIRevision=0.0,
%	AUVSIDescription={Created},
%	AUVSIAuthor={Kameron Eves},
%	AUVSIChecker={[Checker]},
	AUVSILogoPath={./figs/logo.pdf},
%	AUVSIDocID={AF-004}
}

% include extra packages, if needed
\usepackage{makecell}
\usepackage{multirow}

% Remove Heading Numbers
\setcounter{secnumdepth}{0}

\begin{document}
\begin{AUVSITitlePage}
\begin{artifacttable}
	\entry{TW-002, 0.1, 12-03-2018, Created, Kameron Eves, [Checker]}
\end{artifacttable}
\end{AUVSITitlePage}


\section{ Introduction}

Each year, the Association for Unmanned Vehicle Systems International (AUVSI) hosts a Student Unmanned Aerial Systems (SUAS) competition. This year’s competition will be held June 12th to 15th, 2019 and BYU will be sending a team to compete.

The aircraft entered into the competition are judged primarily on a demonstration of ability to autonomously complete a mission which includes the following tasks:

\begin{itemize}
	\item\textbf{Fly Waypoint Path} - Fly waypoints given to the the team just prior to the competition. In this process, and throughout the entire mission, the aircraft must avoid virtual obstacles and stay within boundaries (both horizontal and vertical).
	\item\textbf{Visual Target Classification} - Capture an image of several targets within a search area and report to the judges the shape, color, alpha-numeric character, alpha-numeric character color, and geolocation of each target.
	\item\textbf{Payload Delivery} - Drop on a specified location, an unmanned ground vehicle (UGV) that itself carries a small water bottle. Then, carrying its the water bottle, the UGV must drive to a second specified location.
\end{itemize}

In order, to accomplish these tasks our team has decided upon the following objective statement: 
 
\begin{quote}
Improve upon last year’s BYU AUVSI unmanned aerial system (UAS) by improving path planning, obstacle avoidance, visual object detection, and payload delivery by April 1, 2019 with a budget of \$3,500 and 2,500 man hours.
\end{quote}

We have stipulated several key success measures which are enumerated in Table~\ref{tab:key_measures}. Additional market requirements, performance measures, ideal values, and target values are included in RM-001.

\begin{table}[H]
	\centering
	\caption{Key success measures for the UAS}\label{tab:key_measures}
\begin{tabular}{|P{7.1cm}|>{\centering\arraybackslash}P{0.75cm}|>{\centering\arraybackslash}P{0.75cm}>{\centering\arraybackslash}P{0.75cm}>{\centering\arraybackslash}P{0.75cm}|>{\centering\arraybackslash}P{0.75cm}>{\centering\arraybackslash}P{0.75cm}>{\centering\arraybackslash}P{0.75cm}|}
	\multicolumn{8}{c}{\textbf{\underline{Key}}} \\
		\multicolumn{8}{l}{\textbf{Performance:} \textbf{SG} = Stretch Goal, \textbf{A} = Excellent,	\textbf{B} = Good,	\textbf{C} = Fair}  \\
		\multicolumn{8}{l}{\textbf{Acceptable:} \textbf{L} = Lower, \textbf{I} = Ideal,	\textbf{U} = Upper}\\
	\hline
	\rowcolor[HTML]{C0C0C0}
	 & \multicolumn{4}{c|}{\textbf{Performance}} &\multicolumn{3}{c|}{\textbf{Acceptable}}  \\\rowcolor[HTML]{C0C0C0} 
	\multirow{-2}{*}{ \textbf{Measures (units)}} &{\textbf{SG}} & {\textbf{A}} & {\textbf{B}} & {\textbf{C}} & {\textbf{L}} & {\textbf{I}} & {\textbf{U}} \\
	
	\hline
	\textbf{Obstacles Hit (\#)} & 0 & 1 & 3 & 5 & 0 & 0 & 5 \\
	\hline
	\textbf{Average Waypoint Proximity (ft)} & 5 & 20 & 25 & 30 & 0 & 0 & 100 \\
	\hline
	\textbf{Characteristics Identified (\%)} & 80 & 40 & 30 & 20 & 20 & 100 & 100 \\
	\hline
	\textbf{Airdrop Accuracy (ft)} & 5 & 25 & 50 & 75 & 0 & 0 & 75 \\
	\hline
	\textbf{Number of Manual Takeovers (\#)} & 0 & 1 & 2 & 3 & 0 & 0 & 3 \\
	\hline
\end{tabular}
\end{table}


\section{Description of Design}
\subsection{Airframe}
\subsection{Visual Target Classification}
This year's vision team is changing our system architecture for classifying targets which will
allow for better communication and organization. Instead of downloading each image and image state
onto someone's personal computer, the computer oboard the plane will send image and vehicle state
data to a server on the ground. This server will have a compiled database of all images captured
and will attach classification data onto each image as it is manually processed. Our
autonomous detection script will also be querying the server image database and classifying
images. One team member will be monitoring the autonomous output ready to kill the
program if it is sending too many false positives (which cause the team to incur a
penalty).
\subsection{Payload Delivery}
\section{Summary of Expected Performance}
\subsection{Airframe}
\subsection{Visual Target Classification}
The new server system architecture is expected to perform much better for target classification that preveious year's methods. We are
confident in our ability to send images from the camera to our onboard computer and down to our groundstation.

The autonomous classification system is the largest undertaking of this year's vision subteam. Each of the 6 characteristics we are
required to identify could potentially be done using a different method. We expect to be able to reliable classify fifty percent of
targets autonomously.
\subsection{Payload Delivery}
\section{Status and Future Plans}
\subsection{Airframe}
\subsection{Visual Target Classification}
We have already built much of the server system architecture. There is a strong framework in place for saving and accessing pictures
from the server. We have also constructed a draft of the user interface that contacts the server and requests images and sends back
cropped and classified images back to the server. The system for manual target recognition is already mostly complete.

We have been focusing on first achieving the ability for manual target recognition, but we have also been investing some effort into developing
the autonomous target detection and classification system. As was mentioned previously, the system must be capable of detecting a target
within a frame, geolocating it, and classifying its shape, shape color, alphanumeric, and alphanumeric color. Thus far, we have developed a 
system capable of detecting targets with around 70\% accuracy. We have also modified a deep learning-based character recognition system
to detect synthetic letters with over 90\% accuracy. In the future, we will be working on testing the character recognizer on
real images of targets. We will also be devoloping the shape classifier and color recognition systems.
\subsection{Payload Delivery}
\section{Conclusion}

% document contents
\end{document}
