\documentclass[]{auvsi_doc}
\setkeys{auvsi_doc.cls}{
	AUVSITitle={Design Summary},
%	AUVSIRevision=0.0,
%	AUVSIDescription={Created},
%	AUVSIAuthor={Kameron Eves},
%	AUVSIChecker={[Checker]},
	AUVSILogoPath={./figs/logo.pdf},
%	AUVSIDocID={AF-004}
}

% include extra packages, if needed
\usepackage{makecell}

% Remove Heading Numbers
\setcounter{secnumdepth}{0}

\begin{document}
\begin{AUVSITitlePage}
\begin{artifacttable}
	\entry{TW-002, 0.1, 12-03-2018, Created, Kameron Eves, [Checker]}
\end{artifacttable}
\end{AUVSITitlePage}

\section{Introduction}
\subsection{Project Description}

Each year, the Association for Unmanned Vehicle Systems International (AUVSI) hosts a Student Unmanned Aerial Systems (SUAS) competition. While each year’s competition has unique challenges, the general challenge is to build an Unmanned Aerial System (UAS) capable of autonomous flight, object detection, and payload delivery. This year’s competition will be held June 12th to 15th, 2019 at the Naval Air Station in Patuxent River, Maryland.

The UAS's entered into the competition are judged primarily on their mission success during the competition. This year's mission begins when the team hands control of the aircraft to the autopilot. The autopilot will then initiate the takeoff and attempt to execute the following tasks.

\begin{itemize}
	\item\textbf{Fly Waypoint Path} - Fly waypoints given to the the team just prior to the competition. In this process, and throughout the entire mission, the aircraft must avoid virtual obstacles and stay within boundaries (both horizontal and vertical).
	\item\textbf{Visual Target Classification} - Within a prescribed search zone there will be a number of large cardboard shapes with an alpha-numeric character on them. The aircraft must capture an image of these shaps
	\item\textbf{Payload Delivery} -
\end{itemize}

In general, every aspect of the mission should be completed autonomously.

For the last two years BYU has sponsored an AUVSI team to compete in the competition. The 2017 team was primarily volunteer based and placed 10th overall while the 2018 team was a Capstone team and placed 9th overall. This year’s team is also a Capstone team consisting of BYU Mechanical, Electrical, and Computer Engineering students and looks to place as one of the top five teams.
\section{Description of Design}
\subsection{Airframe}
\subsection{Visual Target Classification}
This year's vision team is changing our system architecture for classifying targets which will
allow for better communication and organization. Instead of downloading each image and image state
onto someone's personal computer, the computer oboard the plane will send image and vehicle state
data to a server on the ground. This server will have a compiled database of all images captured
and will attach classification data onto each image as it is manually processed. Our
autonomous detection script will also be querying the server image database and classifying
images. One team member will be monitoring the autonomous output ready to kill the
program if it is sending too many false positives (which cause the team to incur a
penalty).
\subsection{Payload Delivery}
\section{Summary of Expected Performance}
\subsection{Airframe}
\subsection{Visual Target Classification}
The new server system architecture is expected to perform much better for target classification that preveious year's methods. We are
confident in our ability to send images from the camera to our onboard computer and down to our groundstation.

The autonomous classification system is the largest undertaking of this year's vision subteam. Each of the 6 characteristics we are
required to identify could potentially be done using a different method. We expect to be able to reliable classify fifty percent of
targets autonomously.
\subsection{Payload Delivery}
\section{Status and Future Plans}
\subsection{Airframe}
\subsection{Visual Target Classification}
We have already built much of the server system architecture. There is a strong framework in place for saving and accessing pictures
from the server. We have also constructed a draft of the user interface that contacts the server and requests images and sends back
cropped and classified images back to the server. The system for manual target recognition is already mostly complete.

We have been focusing on first achieving the ability for manual target recognition, but we have also been investing some effort into developing
the autonomous target detection and classification system. As was mentioned previously, the system must be capable of detecting a target
within a frame, geolocating it, and classifying its shape, shape color, alphanumeric, and alphanumeric color. Thus far, we have developed a 
system capable of detecting targets with around 70\% accuracy. We have also modified a deep learning-based character recognition system
to detect synthetic letters with over 90\% accuracy. In the future, we will be working on testing the character recognizer on
real images of targets. We will also be devoloping the shape classifier and color recognition systems.
\subsection{Payload Delivery}
\section{Conclusion}

% document contents
\end{document}
